# PySpark transformations

## Overview
This project loads the provided CSV datasets, apply the specified Spark transformations, and save the result as a single CSV file.


## Project setup

### 1. Create a Virtual environment
This project uses `.venv` as the default virtual environment folder.

> CLI setup can be done by desired IDE like [PyCharm](https://www.jetbrains.com/help/pycharm/creating-virtual-environment.html)
```shell
# Create venv in a specific folder (for this project in the root folder: /pyspark-transformations)
python3 -m venv myvenv

# Activate venv
source venv/bin/activate
```

### 2. Install dependencies
Project dependencies are managed by [UV - Python packaging and dependency management tool](https://docs.astral.sh/uv/). \
Inside the already activated venv run CLI commands:

```shell
# Install UV with pip
pip install uv

# Install dependencies with UV
uv sync
```

This project relies on the following main Python packages:

- pyspark â€” for Spark DataFrame processing
- requests â€” for HTTP calls to external hash API
- python-dotenv â€” to load environment variables
- pytest â€” for running tests


### 3. Setup environment variables
Configuration is handled via environment variables loaded from `.env` file.
Create in project root directory `.env` file according to `.env.example` file. \
Fill in the required variables with appropriate values. For example:

```env
INPUT_DIR=data
INPUT_CLAIMS_CSV=claims_data.csv
```


## Project Structure
The project is organized into multiple files.

```bash
pyspark-transformations/
â”‚
â”œâ”€â”€ .env.example                    # Environment variables example
â”œâ”€â”€ config.py                       # Configuration file
â”œâ”€â”€ main.py                         # Main script to run the transformation pipeline
â”œâ”€â”€ pyproject.toml                  # Project dependencies
â”œâ”€â”€ uv.lock                         # Lock file generated by UV
â”œâ”€â”€ README.md                   
â”œâ”€â”€ CHANGELOG.md      
â”œâ”€â”€ version.py                      # Project version info
â”œâ”€â”€ data/                           # Input CSV files
â”‚   â”œâ”€â”€ claims_data.csv       
â”‚   â”œâ”€â”€ policyholder_data.csv             
â”œâ”€â”€ logs                            # Log file generated during execution
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ processed_claims.csv        # Output file
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transformations.py          # Data transformation functions
â”‚   â”œâ”€â”€ hash_utils.py               # Functions to fetch and handle claim_id hashes
â”‚   â”œâ”€â”€ utils.py                    # Utility functions for saving single CSV file
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_transformations.py    
```


## Running the project
Example command to run pipeline: `python main.py`

The main script performs the following steps:
- Creates a PySpark session.
- Reads input CSV files containing claims and policyholder data and loads the data into Spark DataFrames.
- Applies the required transformations to the data, such as extracting claim types, claim periods, and generating hashes.
- Saves the processed data into a single CSV output file.



### Error Handling & Logging
Logging is set up for monitoring pipeline execution, and unit tests ensure the correctness of transformation logic. 
The logging configuration is set to INFO level, and every step of the process is logged for easier debugging and monitoring.


## Tests
This project includes unit tests to verify each transformation function and utility.

- Tests are written using `pytest`.
- To run all tests, simply execute:

```bash
pytest tests/
```


## Assumptions

- Input CSV files exist in the specified input directory and match expected schema.

- External API for MD4 hashing is reachable and responds with a JSON containing a Digest field.

- The environment variables are properly set in .env.

- Spark is correctly installed and Java runtime is configured.

- The pipeline uses an inner join between claims and policyholder datasets on policyholder_id.
  This means only claims with matching policyholder records will be included in the output.

- The claim_type is derived from the prefix of claim_id. If the prefix is not "CL" or "RX", the claim_type will be assigned NULL value.

- The output is a single CSV file without extra Spark metadata files.

---

Made with ğŸ’œ and â˜•ï¸ by Denisa MensatorisovÃ¡
